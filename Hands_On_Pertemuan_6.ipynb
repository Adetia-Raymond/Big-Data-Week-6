{"cells":[{"cell_type":"markdown","id":"f89f7684","metadata":{"id":"f89f7684"},"source":["# Hands-On Pertemuan 6: Data Processing dengan Apache Spark"]},{"cell_type":"markdown","id":"e30ce9d1","metadata":{"id":"e30ce9d1"},"source":["## Tujuan:\n","- Memahami dan mempraktikkan data processing menggunakan Apache Spark.\n","- Menggunakan Spark untuk operasi data yang efisien pada dataset besar.\n","- Menerapkan teknik canggih dalam Spark untuk mengatasi kasus penggunaan nyata."]},{"cell_type":"markdown","id":"cd5c2f90","metadata":{"id":"cd5c2f90"},"source":["### 1. Pengenalan Spark DataFrames\n","Spark DataFrame menyediakan struktur data yang optimal dengan operasi yang dioptimalkan untuk pemrosesan data besar, yang sangat mirip dengan DataFrame di Pandas atau di RDBMS.\n","\n","- **Tugas 1**: Buat DataFrame sederhana di Spark dan eksplorasi beberapa fungsi dasar yang tersedia."]},{"cell_type":"markdown","source":[],"metadata":{"id":"bijM0UHAHnTG"},"id":"bijM0UHAHnTG"},{"cell_type":"code","source":["!pip install pyspark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uD8GWab9Hnp8","executionInfo":{"status":"ok","timestamp":1727786872935,"user_tz":-420,"elapsed":52162,"user":{"displayName":"Adetia Raymond S.","userId":"12173824071937058668"}},"outputId":"2bdd4681-6ec0-4fdd-b1a7-489081a18dca"},"id":"uD8GWab9Hnp8","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark\n","  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840625 sha256=172182ba13f60c499ca176e877596b18105fb709dcfb0383d4e3bde03e6236b0\n","  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.5.3\n"]}]},{"cell_type":"code","execution_count":7,"id":"986d01c7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"986d01c7","executionInfo":{"status":"ok","timestamp":1727786908630,"user_tz":-420,"elapsed":21845,"user":{"displayName":"Adetia Raymond S.","userId":"12173824071937058668"}},"outputId":"78a30a91-0936-46df-9405-d0e8fc100f9d"},"outputs":[{"output_type":"stream","name":"stdout","text":["+------------+----------+------+\n","|EmployeeName|Department|Salary|\n","+------------+----------+------+\n","|       James|     Sales|  3000|\n","|     Michael|     Sales|  4600|\n","|      Robert|     Sales|  4100|\n","|       Maria|   Finance|  3000|\n","+------------+----------+------+\n","\n"]}],"source":["# Contoh membuat DataFrame sederhana dan operasi dasar\n","\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName('HandsOnPertemuan6').getOrCreate()\n","\n","data = [('James', 'Sales', 3000),\n","        ('Michael', 'Sales', 4600),\n","        ('Robert', 'Sales', 4100),\n","        ('Maria', 'Finance', 3000)]\n","columns = ['EmployeeName', 'Department', 'Salary']\n","\n","df = spark.createDataFrame(data, schema=columns)\n","df.show()\n"]},{"cell_type":"markdown","id":"fca66b73","metadata":{"id":"fca66b73"},"source":["### 2. Transformasi Dasar dengan DataFrames\n","Pemrosesan data meliputi transformasi seperti filtering, selections, dan aggregations. Spark menyediakan cara efisien untuk melaksanakan operasi ini.\n","\n","- **Tugas 2**: Gunakan operasi filter, select, groupBy untuk mengekstrak informasi dari data, serta lakukan agregasi data untuk mendapatkan insight tentang dataset menggunakan perintah seperti mean, max, sum."]},{"cell_type":"code","execution_count":16,"id":"58232678","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"58232678","executionInfo":{"status":"ok","timestamp":1727787967237,"user_tz":-420,"elapsed":4967,"user":{"displayName":"Adetia Raymond S.","userId":"12173824071937058668"}},"outputId":"b6089a10-6a6b-4ff8-8dff-7dcca42c4b95"},"outputs":[{"output_type":"stream","name":"stdout","text":["+------------+------+\n","|EmployeeName|Salary|\n","+------------+------+\n","|       James|  3000|\n","|     Michael|  4600|\n","|      Robert|  4100|\n","|       Maria|  3000|\n","+------------+------+\n","\n","+------------+----------+------+\n","|EmployeeName|Department|Salary|\n","+------------+----------+------+\n","|     Michael|     Sales|  4600|\n","|      Robert|     Sales|  4100|\n","+------------+----------+------+\n","\n","+----------+-----------+\n","|Department|avg(Salary)|\n","+----------+-----------+\n","|     Sales|     3900.0|\n","|   Finance|     3000.0|\n","+----------+-----------+\n","\n","Aggregated Data (grouped by Department):\n","+----------+--------------+----------+------------+\n","|Department|Average_Salary|Max_Salary|Total_Salary|\n","+----------+--------------+----------+------------+\n","|     Sales|        3900.0|      4600|       11700|\n","|   Finance|        3000.0|      3000|        3000|\n","+----------+--------------+----------+------------+\n","\n"]}],"source":["# Contoh operasi transformasi DataFrame\n","from pyspark.sql.functions import col, mean, max, sum\n","\n","df.select('EmployeeName', 'Salary').show()\n","df.filter(col('Salary') > 3000).show()\n","df.groupBy('Department').avg('Salary').show()\n","\n","aggregated_df = df.groupBy(\"Department\").agg(\n","    mean(\"Salary\").alias(\"Average_Salary\"),\n","    max(\"Salary\").alias(\"Max_Salary\"),\n","    sum(\"Salary\").alias(\"Total_Salary\")\n",")\n","print(\"Aggregated Data (grouped by Department):\")\n","aggregated_df.show()"]},{"cell_type":"markdown","id":"89763d36","metadata":{"id":"89763d36"},"source":["### 3. Bekerja dengan Tipe Data Kompleks\n","Spark mendukung tipe data yang kompleks seperti maps, arrays, dan structs yang memungkinkan operasi yang lebih kompleks pada dataset yang kompleks.\n","\n","- **Tugas 3**: Eksplorasi bagaimana mengolah tipe data kompleks dalam Spark DataFrames."]},{"cell_type":"code","execution_count":18,"id":"14701d79","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"14701d79","outputId":"c12f232f-7cdd-4220-ffb9-760f95fcf3c3","executionInfo":{"status":"ok","timestamp":1727788339897,"user_tz":-420,"elapsed":1733,"user":{"displayName":"Adetia Raymond S.","userId":"12173824071937058668"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["+------------+----------+------+-----------+-----------------+\n","|EmployeeName|Department|Salary|SalaryBonus|TotalCompensation|\n","+------------+----------+------+-----------+-----------------+\n","|       James|     Sales|  3000|      300.0|           3300.0|\n","|     Michael|     Sales|  4600|      460.0|           5060.0|\n","|      Robert|     Sales|  4100|      410.0|           4510.0|\n","|       Maria|   Finance|  3000|      300.0|           3300.0|\n","+------------+----------+------+-----------+-----------------+\n","\n","+------------+----------+------+-----------+\n","|EmployeeName|Department|Salary|SalaryBonus|\n","+------------+----------+------+-----------+\n","|       James|     Sales|  3000|      300.0|\n","|     Michael|     Sales|  4600|      460.0|\n","|      Robert|     Sales|  4100|      410.0|\n","|       Maria|   Finance|  3000|      300.0|\n","+------------+----------+------+-----------+\n","\n"]}],"source":["# Create SalaryBonus as 10% of Salary\n","df = df.withColumn('SalaryBonus', df['Salary'] * 0.1)\n","\n","# Now you can calculate TotalCompensation\n","df.withColumn('TotalCompensation', df['Salary'] + df['SalaryBonus']).show()\n","\n","df.show()\n"]},{"cell_type":"markdown","id":"5b3b58dd","metadata":{"id":"5b3b58dd"},"source":["### 4. Operasi Data Lanjutan\n","Menggunakan Spark untuk operasi lanjutan seperti window functions, user-defined functions (UDFs), dan mengoptimalkan query.\n","\n","- **Tugas 4**: Implementasikan window function untuk menghitung running totals atau rangkings."]},{"cell_type":"code","execution_count":24,"id":"035312eb","metadata":{"id":"035312eb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727790136435,"user_tz":-420,"elapsed":3009,"user":{"displayName":"Adetia Raymond S.","userId":"12173824071937058668"}},"outputId":"33d0100d-b63c-4413-ccd4-15d59c8c7835"},"outputs":[{"output_type":"stream","name":"stdout","text":["+------------+----------+------+-----------+----+\n","|EmployeeName|Department|Salary|SalaryBonus|Rank|\n","+------------+----------+------+-----------+----+\n","|       Maria|   Finance|  3000|      300.0|   1|\n","|       James|     Sales|  3000|      300.0|   1|\n","|      Robert|     Sales|  4100|      410.0|   2|\n","|     Michael|     Sales|  4600|      460.0|   3|\n","+------------+----------+------+-----------+----+\n","\n","+------------+----------+------+-----------+-------------+\n","|EmployeeName|Department|Salary|SalaryBonus|Running_Total|\n","+------------+----------+------+-----------+-------------+\n","|       Maria|   Finance|  3000|      300.0|         3000|\n","|       James|     Sales|  3000|      300.0|         3000|\n","|      Robert|     Sales|  4100|      410.0|         7100|\n","|     Michael|     Sales|  4600|      460.0|        11700|\n","+------------+----------+------+-----------+-------------+\n","\n"]}],"source":["# Contoh menggunakan window functions\n","from pyspark.sql.window import Window\n","from pyspark.sql import functions as F\n","\n","windowSpec = Window.partitionBy('Department').orderBy('Salary')\n","df.withColumn('Rank', F.rank().over(windowSpec)).show()\n","df.withColumn('Running_Total', F.sum('Salary').over(windowSpec)).show()\n","\n"]},{"cell_type":"markdown","id":"f8a097ec","metadata":{"id":"f8a097ec"},"source":["### 5. Kesimpulan dan Eksplorasi Lebih Lanjut\n","Review apa yang telah dipelajari tentang pemrosesan data menggunakan Spark dan eksplorasi teknik lebih lanjut untuk mengoptimalkan pemrosesan data Anda.\n","- **Tugas 5**: Buat ringkasan dari semua operasi yang telah dilakukan dan bagaimana teknik ini dapat diterapkan pada proyek data Anda."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}